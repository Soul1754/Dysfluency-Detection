{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e9589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "  audio_file      word  start_time  end_time  stutter_type  \\\n",
      "0      data1  \"really\"         0.0       4.0        fluent   \n",
      "1      data2       \"I\"         0.0       0.8    repetition   \n",
      "2      data3  \"really\"         0.0       1.5         block   \n",
      "3      data4  \"really\"         0.5       1.5  prolongation   \n",
      "\n",
      "                               transcript (optional)         audio_path  \n",
      "0  \"I really love playing the guitar in the eveni...  ..\\data\\data1.mp3  \n",
      "1  \"I really love playing the guitar in the eveni...  ..\\data\\data2.mp3  \n",
      "2  \"I really love playing the guitar in the eveni...  ..\\data\\data3.mp3  \n",
      "3  \"I really love playing the guitar in the eveni...  ..\\data\\data4.mp3  \n",
      "Total samples: 4\n",
      "\n",
      "Label mapping:\n",
      "{'fluent': 0, 'repetition': 1, 'prolongation': 2, 'block': 3}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Read csv\n",
    "df = pd.read_csv(\"../annotations.csv\")\n",
    "\n",
    "# Add full path to audio files (assuming they're in ../data folder)\n",
    "data_dir = Path(\"../data\")\n",
    "df['audio_path'] = df['audio_file'].apply(lambda x: str(data_dir / f\"{x}.mp3\"))\n",
    "\n",
    "print(\"Dataset preview:\")\n",
    "print(df.head())\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "# Utility: Load audio and extract segments per word/event\n",
    "def extract_event(audio_path, start, end):\n",
    "    \"\"\"Load MP3 audio and extract a segment\"\"\"\n",
    "    try:\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        # Resample to 16kHz if needed\n",
    "        if sr != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "            waveform = resampler(waveform)\n",
    "            sr = 16000\n",
    "        \n",
    "        # Convert start/end times to sample indices\n",
    "        start_sample = int(start * sr)\n",
    "        end_sample = int(end * sr)\n",
    "        segment = waveform[:, start_sample:end_sample]\n",
    "        return segment.squeeze()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Map event type to integer label\n",
    "label_map = {'fluent': 0, 'repetition': 1, 'prolongation': 2, 'block': 3}\n",
    "df['label'] = df['stutter_type'].map(label_map)\n",
    "\n",
    "print(\"\\nLabel mapping:\")\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c6c3bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 4 samples\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class StutterEventDataset(Dataset):\n",
    "    \"\"\"Dataset class for stuttering event classification\"\"\"\n",
    "    def __init__(self, df, processor):\n",
    "        self.df = df\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Extract audio segment\n",
    "        audio_segment = extract_event(row['audio_path'], row['start_time'], row['end_time'])\n",
    "        \n",
    "        if audio_segment is None:\n",
    "            # Return dummy data if loading fails\n",
    "            input_values = torch.zeros(16000)\n",
    "            label = row['label']\n",
    "        else:\n",
    "            # Process audio through processor\n",
    "            input_values = self.processor(\n",
    "                audio_segment.numpy(), \n",
    "                sampling_rate=16000, \n",
    "                return_tensors=\"pt\"\n",
    "            ).input_values.squeeze()\n",
    "        \n",
    "        label = torch.tensor(row['label'], dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            \"input_values\": input_values, \n",
    "            \"labels\": label\n",
    "        }\n",
    "\n",
    "# Create dataset instance\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "dataset = StutterEventDataset(df, processor)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25bbeee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and ready for fine-tuning\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model for sequence classification (for classifying entire audio segments)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"facebook/wav2vec2-base-960h\",\n",
    "    num_labels=4,  # 4 stutter event types: fluent, repetition, prolongation, block\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "print(\"Model loaded and ready for fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0768f1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 3\n",
      "Validation samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pruthviraj Gaikwad\\AppData\\Local\\Temp\\ipykernel_43356\\2399735469.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:43, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.390625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.390625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.390625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.390625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.390625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.390625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.390625</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.404297</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.404297</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.417969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.423828</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.432617</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.440430</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.447266</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.451172</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.455078</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.460938</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.459961</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.389100</td>\n",
       "      <td>1.455078</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.362200</td>\n",
       "      <td>1.455078</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=1.3756510257720946, metrics={'train_runtime': 43.5856, 'train_samples_per_second': 1.377, 'train_steps_per_second': 0.459, 'total_flos': 2178883676160000.0, 'train_loss': 1.3756510257720946, 'epoch': 20.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Split dataset into train and validation sets\n",
    "train_indices, val_indices = train_test_split(\n",
    "    range(len(df)), \n",
    "    test_size=0.2,  # 20% for validation\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "df_train = df.iloc[train_indices].reset_index(drop=True)\n",
    "df_val = df.iloc[val_indices].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training samples: {len(df_train)}\")\n",
    "print(f\"Validation samples: {len(df_val)}\")\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = StutterEventDataset(df_train, processor)\n",
    "eval_dataset = StutterEventDataset(df_val, processor)\n",
    "\n",
    "# Compute metrics for evaluation (so Trainer produces eval_accuracy)\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions\n",
    "    # some models return a tuple (logits, hidden_states,...)\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    # handle both regression and classification logits\n",
    "    if preds is None:\n",
    "        return {}\n",
    "    if preds.ndim > 1:\n",
    "        pred_labels = np.argmax(preds, axis=1)\n",
    "    else:\n",
    "        pred_labels = preds\n",
    "    return {\"accuracy\": accuracy_score(p.label_ids, pred_labels)}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,  # Small batch size for small dataset\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=20,  # More epochs for small dataset\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "# Create trainer (attach compute_metrics so 'eval_accuracy' will be available)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796e9d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on validation set...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Results:\n",
      "eval_loss: 1.390625\n",
      "eval_accuracy: 0.0\n",
      "eval_runtime: 1.0395\n",
      "eval_samples_per_second: 0.962\n",
      "eval_steps_per_second: 0.962\n",
      "epoch: 20.0\n",
      "\n",
      "==================================================\n",
      "Sample Predictions\n",
      "==================================================\n",
      "True label: repetition\n",
      "Predicted label: prolongation\n",
      "Confidence scores: [0.24441483616828918, 0.24895991384983063, 0.27545788884162903, 0.23116734623908997]\n",
      "\n",
      "Model saved to ../models/wav2vec2_stutter_classifier\n"
     ]
    }
   ],
   "source": [
    "# Create reverse mapping\n",
    "reverse_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nValidation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Make predictions on a sample\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Sample Predictions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sample_idx = 0\n",
    "sample = eval_dataset[sample_idx]\n",
    "input_values = sample[\"input_values\"].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_values)\n",
    "    logits = outputs.logits\n",
    "    predicted_label_id = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = reverse_label_map[predicted_label_id]\n",
    "    true_label = reverse_label_map[sample[\"labels\"].item()]\n",
    "    \n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(f\"Predicted label: {predicted_label}\")\n",
    "    print(f\"Confidence scores: {torch.softmax(logits, dim=-1)[0].tolist()}\")\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"../models/wav2vec2_stutter_classifier\")\n",
    "processor.save_pretrained(\"../models/wav2vec2_stutter_classifier\")\n",
    "print(\"\\nModel saved to ../models/wav2vec2_stutter_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bd16a",
   "metadata": {},
   "source": [
    "## Inference: Detecting Stuttering in Continuous Speech\n",
    "\n",
    "Now we can use the trained model to detect stuttering in a full MP3 file. This involves:\n",
    "1. Loading the full audio file\n",
    "2. Transcribing it with wav2vec2 to get word boundaries (using CTC alignment)\n",
    "3. Extracting segments for each word\n",
    "4. Classifying each word segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2205b882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions loaded for word-level stuttering detection\n"
     ]
    }
   ],
   "source": [
    "# Load the base wav2vec2 model for transcription and CTC alignment\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import re\n",
    "\n",
    "# Load a fresh processor and ASR model for getting word boundaries\n",
    "asr_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "asr_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
    "asr_model.eval()\n",
    "\n",
    "def get_word_boundaries(audio_path):\n",
    "    \"\"\"\n",
    "    Get word-level time boundaries from an audio file using CTC alignment.\n",
    "    Returns a list of dictionaries with word and time boundaries.\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(audio_path)\n",
    "    if len(waveform.shape) > 1:\n",
    "        waveform = waveform[0:1]\n",
    "    \n",
    "    # Resample to 16kHz\n",
    "    if sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sr, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "        sr = 16000\n",
    "    \n",
    "    # Get model inputs\n",
    "    inputs = asr_processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_values = inputs.input_values.to(device)\n",
    "    \n",
    "    # Get CTC output\n",
    "    with torch.no_grad():\n",
    "        outputs = asr_model(input_values)\n",
    "    \n",
    "    # Get logits and decode\n",
    "    logits = outputs.logits[0].cpu()\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Get transcription\n",
    "    transcription = asr_processor.decode(predicted_ids)\n",
    "    \n",
    "    # Simple approach: split by spaces to get words\n",
    "    words = transcription.split()\n",
    "    \n",
    "    # Calculate frame-to-time ratio\n",
    "    num_frames = logits.shape[0]\n",
    "    total_duration = waveform.shape[1] / sr\n",
    "    frame_duration = total_duration / num_frames\n",
    "    \n",
    "    # Rough estimation of word boundaries based on frame count\n",
    "    # For better alignment, consider using Montreal Forced Aligner\n",
    "    word_boundaries = []\n",
    "    chars_processed = 0\n",
    "    current_time = 0.0\n",
    "    \n",
    "    for word in words:\n",
    "        # Estimate duration based on word length\n",
    "        word_duration = len(word) * 0.08  # ~80ms per character (rough estimate)\n",
    "        word_boundaries.append({\n",
    "            'word': word,\n",
    "            'start_time': current_time,\n",
    "            'end_time': current_time + word_duration,\n",
    "        })\n",
    "        current_time += word_duration\n",
    "    \n",
    "    return word_boundaries, waveform, sr\n",
    "\n",
    "print(\"Functions loaded for word-level stuttering detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0d6a18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained stuttering classifier loaded\n"
     ]
    }
   ],
   "source": [
    "# Load the trained stuttering classifier\n",
    "trained_model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"../models/wav2vec2_stutter_classifier\"\n",
    ").to(device)\n",
    "trained_model.eval()\n",
    "\n",
    "classifier_processor = Wav2Vec2Processor.from_pretrained(\n",
    "    \"../models/wav2vec2_stutter_classifier\"\n",
    ")\n",
    "\n",
    "print(\"Trained stuttering classifier loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6fb10519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stuttering detection function ready\n"
     ]
    }
   ],
   "source": [
    "def detect_stuttering_in_speech(audio_path):\n",
    "    \"\"\"\n",
    "    Detect stuttering at word level in a speech audio file.\n",
    "    \n",
    "    Returns:\n",
    "    - results: List of dicts with word, time boundaries, and stuttering classification\n",
    "    - waveform: Audio waveform\n",
    "    - sr: Sample rate\n",
    "    \"\"\"\n",
    "    # Get word boundaries\n",
    "    word_boundaries, waveform, sr = get_word_boundaries(audio_path)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for word_info in word_boundaries:\n",
    "        word = word_info['word']\n",
    "        start_time = word_info['start_time']\n",
    "        end_time = word_info['end_time']\n",
    "        \n",
    "        try:\n",
    "            # Extract audio segment for this word\n",
    "            start_sample = int(start_time * sr)\n",
    "            end_sample = int(end_time * sr)\n",
    "            \n",
    "            # Ensure boundaries are valid\n",
    "            if end_sample > waveform.shape[1]:\n",
    "                end_sample = waveform.shape[1]\n",
    "            if start_sample >= end_sample:\n",
    "                continue\n",
    "            \n",
    "            audio_segment = waveform[:, start_sample:end_sample].squeeze()\n",
    "            \n",
    "            # Skip very short segments\n",
    "            if audio_segment.shape[0] < sr * 0.05:  # Less than 50ms\n",
    "                results.append({\n",
    "                    'word': word,\n",
    "                    'start_time': start_time,\n",
    "                    'end_time': end_time,\n",
    "                    'stutter_type': 'unknown',\n",
    "                    'confidence': 0.0,\n",
    "                    'error': 'segment too short'\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Process through classifier\n",
    "            inputs = classifier_processor(\n",
    "                audio_segment.numpy(),\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            input_values = inputs.input_values.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = trained_model(input_values)\n",
    "                logits = outputs.logits[0]\n",
    "                probabilities = torch.softmax(logits, dim=-1)\n",
    "                predicted_label_id = torch.argmax(probabilities).item()\n",
    "                confidence = probabilities[predicted_label_id].item()\n",
    "            \n",
    "            stutter_type = reverse_label_map[predicted_label_id]\n",
    "            \n",
    "            results.append({\n",
    "                'word': word,\n",
    "                'start_time': round(start_time, 2),\n",
    "                'end_time': round(end_time, 2),\n",
    "                'stutter_type': stutter_type,\n",
    "                'confidence': round(confidence, 3),\n",
    "                'all_confidences': {\n",
    "                    label: round(prob.item(), 3) \n",
    "                    for label, prob in zip(reverse_label_map.values(), probabilities)\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing word '{word}': {e}\")\n",
    "            results.append({\n",
    "                'word': word,\n",
    "                'start_time': start_time,\n",
    "                'end_time': end_time,\n",
    "                'stutter_type': 'error',\n",
    "                'confidence': 0.0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results, waveform, sr\n",
    "\n",
    "print(\"Stuttering detection function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8789ed88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ../data/data4.mp3\n",
      "\n",
      "================================================================================\n",
      "STUTTERING DETECTION RESULTS (Dictionary-based)\n",
      "================================================================================\n",
      "Full transcript: I RREALLY LOVE PLAYING THE GUITAR IN THE EVENINGS\n",
      "================================================================================\n",
      "\n",
      "DEBUG: Word validation:\n",
      "--------------------------------------------------------------------------------\n",
      "0: 'I' - ✓ VALID\n",
      "1: 'RREALLY' - ✗ INVALID (likely stutter)\n",
      "2: 'LOVE' - ✓ VALID\n",
      "3: 'PLAYING' - ✓ VALID\n",
      "4: 'THE' - ✓ VALID\n",
      "5: 'GUITAR' - ✓ VALID\n",
      "6: 'IN' - ✓ VALID\n",
      "7: 'THE' - ✓ VALID\n",
      "8: 'EVENINGS' - ✓ VALID\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "⚠ Stuttering detected in 1 word(s):\n",
      "\n",
      "Word                 Likely Intended           Confidence  \n",
      "--------------------------------------------------------------------------------\n",
      "RREALLY              realy                     0.268       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Highlighted transcript:\n",
      "I [RREALLY](STUTTER) LOVE PLAYING THE GUITAR IN THE EVENINGS\n",
      "\n",
      "Summary:\n",
      "  Total words: 9\n",
      "  Valid English words: 8\n",
      "  Stuttering words detected: 1\n",
      "\n",
      "Stuttering words identified:\n",
      "  - 'RREALLY'\n",
      "\n",
      "Full results saved to ../stuttering_detection_results.csv\n",
      "Stuttering instances saved to ../stuttering_instances.csv\n",
      "\n",
      "================================================================================\n",
      "STUTTERING DETECTION RESULTS (Dictionary-based)\n",
      "================================================================================\n",
      "Full transcript: I RREALLY LOVE PLAYING THE GUITAR IN THE EVENINGS\n",
      "================================================================================\n",
      "\n",
      "DEBUG: Word validation:\n",
      "--------------------------------------------------------------------------------\n",
      "0: 'I' - ✓ VALID\n",
      "1: 'RREALLY' - ✗ INVALID (likely stutter)\n",
      "2: 'LOVE' - ✓ VALID\n",
      "3: 'PLAYING' - ✓ VALID\n",
      "4: 'THE' - ✓ VALID\n",
      "5: 'GUITAR' - ✓ VALID\n",
      "6: 'IN' - ✓ VALID\n",
      "7: 'THE' - ✓ VALID\n",
      "8: 'EVENINGS' - ✓ VALID\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "⚠ Stuttering detected in 1 word(s):\n",
      "\n",
      "Word                 Likely Intended           Confidence  \n",
      "--------------------------------------------------------------------------------\n",
      "RREALLY              realy                     0.268       \n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Highlighted transcript:\n",
      "I [RREALLY](STUTTER) LOVE PLAYING THE GUITAR IN THE EVENINGS\n",
      "\n",
      "Summary:\n",
      "  Total words: 9\n",
      "  Valid English words: 8\n",
      "  Stuttering words detected: 1\n",
      "\n",
      "Stuttering words identified:\n",
      "  - 'RREALLY'\n",
      "\n",
      "Full results saved to ../stuttering_detection_results.csv\n",
      "Stuttering instances saved to ../stuttering_instances.csv\n"
     ]
    }
   ],
   "source": [
    "# Install nltk for English dictionary\n",
    "import nltk\n",
    "nltk.download('words', quiet=True)\n",
    "from nltk.corpus import words\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load English dictionary\n",
    "english_words_set = set(words.words())\n",
    "\n",
    "def is_valid_english_word(word):\n",
    "    \"\"\"\n",
    "    Check if a word is valid English using multiple methods\n",
    "    \"\"\"\n",
    "    word_lower = word.lower()\n",
    "    \n",
    "    # Method 1: Check in NLTK dictionary\n",
    "    if word_lower in english_words_set:\n",
    "        return True\n",
    "    \n",
    "    # Method 2: Check if removing repeated characters gives valid word\n",
    "    # (handles cases like \"rrreally\" -> \"really\")\n",
    "    cleaned_word = ''.join([word_lower[i] for i in range(len(word_lower)) \n",
    "                           if i == 0 or word_lower[i] != word_lower[i-1]])\n",
    "    if cleaned_word != word_lower and cleaned_word in english_words_set:\n",
    "        return False  # This is a stuttering variant, mark as invalid\n",
    "    \n",
    "    # Method 3: Use TextBlob for spell checking\n",
    "    try:\n",
    "        blob = TextBlob(word_lower)\n",
    "        # If the word is corrected to something different, it's likely misspelled\n",
    "        corrected = str(blob.correct()).lower()\n",
    "        if corrected == word_lower:\n",
    "            return True\n",
    "        # If it can be corrected, it's likely a stutter\n",
    "        return False\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Example: Run inference on a test audio file\n",
    "test_audio_path = \"../data/data4.mp3\"  # Replace with your test MP3 file\n",
    "\n",
    "# Check if file exists, if not create a demo message\n",
    "import os\n",
    "if os.path.exists(test_audio_path):\n",
    "    print(f\"Processing: {test_audio_path}\")\n",
    "    results, waveform, sr = detect_stuttering_in_speech(test_audio_path)\n",
    "    \n",
    "    all_words = [r['word'] for r in results]\n",
    "    \n",
    "    # SMART DETECTION: Find words that are NOT valid English\n",
    "    # These are likely stuttering transcriptions (e.g., \"rrrrreally\", \"iiiii\")\n",
    "    stuttering_results = []\n",
    "    for r in results:\n",
    "        word_lower = r['word'].lower()\n",
    "        is_valid = is_valid_english_word(r['word'])\n",
    "        \n",
    "        if not is_valid:\n",
    "            stuttering_results.append(r)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STUTTERING DETECTION RESULTS (Dictionary-based)\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Full transcript: {' '.join(all_words)}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # DEBUG: Show which words are valid/invalid English\n",
    "    print(\"\\nDEBUG: Word validation:\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, r in enumerate(results):\n",
    "        is_valid = is_valid_english_word(r['word'])\n",
    "        status = \"✓ VALID\" if is_valid else \"✗ INVALID (likely stutter)\"\n",
    "        print(f\"{i}: '{r['word']}' - {status}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if len(stuttering_results) == 0:\n",
    "        print(\"\\n✓ No stuttering detected. All words are valid English.\")\n",
    "        print(f\"\\nTranscript: {' '.join(all_words)}\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Stuttering detected in {len(stuttering_results)} word(s):\\n\")\n",
    "        print(f\"{'Word':<20} {'Likely Intended':<25} {'Confidence':<12}\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        for result in stuttering_results:\n",
    "            # Try to find the closest valid English word by removing repeated characters\n",
    "            word_lower = result['word'].lower()\n",
    "            # Remove repeated characters to guess the intended word\n",
    "            cleaned_word = ''.join([word_lower[i] for i in range(len(word_lower)) \n",
    "                                  if i == 0 or word_lower[i] != word_lower[i-1]])\n",
    "            \n",
    "            print(f\"{result['word']:<20} {cleaned_word:<25} {result['confidence']:<12}\")\n",
    "        \n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        # Highlight stuttering in transcript\n",
    "        transcript_highlighted = []\n",
    "        for word_result in results:\n",
    "            is_valid = is_valid_english_word(word_result['word'])\n",
    "            if not is_valid:\n",
    "                transcript_highlighted.append(f\"[{word_result['word'].upper()}](STUTTER)\")\n",
    "            else:\n",
    "                transcript_highlighted.append(word_result['word'])\n",
    "        \n",
    "        print(f\"\\nHighlighted transcript:\")\n",
    "        print(\" \".join(transcript_highlighted))\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Total words: {len(results)}\")\n",
    "        print(f\"  Valid English words: {len(results) - len(stuttering_results)}\")\n",
    "        print(f\"  Stuttering words detected: {len(stuttering_results)}\")\n",
    "        \n",
    "        # Show the stuttering words\n",
    "        print(f\"\\nStuttering words identified:\")\n",
    "        for result in stuttering_results:\n",
    "            print(f\"  - '{result['word']}'\")\n",
    "    \n",
    "    # Save full results for reference\n",
    "    import pandas as pd\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"../stuttering_detection_results.csv\", index=False)\n",
    "    print(f\"\\nFull results saved to ../stuttering_detection_results.csv\")\n",
    "    \n",
    "    # Save only stuttering results\n",
    "    if len(stuttering_results) > 0:\n",
    "        stuttering_df = pd.DataFrame(stuttering_results)\n",
    "        stuttering_df.to_csv(\"../stuttering_instances.csv\", index=False)\n",
    "        print(f\"Stuttering instances saved to ../stuttering_instances.csv\")\n",
    "else:\n",
    "    print(f\"Test audio file not found at {test_audio_path}\")\n",
    "    print(\"Please place your MP3 file in the data folder and update the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bef74850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "CORRECTED STUTTERED WORDS\n",
      "====================================================================================================\n",
      "\n",
      "Stuttered Word       Corrected Word       Correction Method              Confidence  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "RREALLY              really               Fuzzy matching                 0.268       \n",
      "----------------------------------------------------------------------------------------------------\n",
      "RREALLY              really               Fuzzy matching                 0.268       \n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Original transcript: I RREALLY LOVE PLAYING THE GUITAR IN THE EVENINGS\n",
      "Corrected transcript: I really LOVE PLAYING THE GUITAR IN THE EVENINGS\n",
      "====================================================================================================\n",
      "\n",
      "Original transcript: I RREALLY LOVE PLAYING THE GUITAR IN THE EVENINGS\n",
      "Corrected transcript: I really LOVE PLAYING THE GUITAR IN THE EVENINGS\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from difflib import get_close_matches\n",
    "from nltk.corpus import words\n",
    "\n",
    "def get_correct_word(stuttered_word):\n",
    "    \"\"\"\n",
    "    Find the grammatically correct version of a stuttered word.\n",
    "    Uses multiple strategies to find the best match.\n",
    "    \"\"\"\n",
    "    word_lower = stuttered_word.lower()\n",
    "    english_words_list = list(english_words_set)\n",
    "    \n",
    "    # Strategy 1: Remove repeated characters and check if it's valid\n",
    "    cleaned_word = ''.join([word_lower[i] for i in range(len(word_lower)) \n",
    "                           if i == 0 or word_lower[i] != word_lower[i-1]])\n",
    "    \n",
    "    if cleaned_word in english_words_set:\n",
    "        return cleaned_word, \"Repeated character removal\"\n",
    "    \n",
    "    # Strategy 2: Use difflib to find closest matches (fuzzy matching)\n",
    "    close_matches = get_close_matches(word_lower, english_words_list, n=1, cutoff=0.6)\n",
    "    \n",
    "    if close_matches:\n",
    "        return close_matches[0], \"Fuzzy matching\"\n",
    "    \n",
    "    # Strategy 3: Try TextBlob spell correction\n",
    "    try:\n",
    "        blob = TextBlob(word_lower)\n",
    "        corrected = str(blob.correct()).lower()\n",
    "        if corrected != word_lower:\n",
    "            return corrected, \"TextBlob spell check\"\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 4: Remove vowels and find similar consonant patterns\n",
    "    def get_consonants(word):\n",
    "        \"\"\"Extract consonants from word\"\"\"\n",
    "        return ''.join([c for c in word if c not in 'aeiou'])\n",
    "    \n",
    "    consonants = get_consonants(word_lower)\n",
    "    if len(consonants) > 2:\n",
    "        for candidate in english_words_list:\n",
    "            if get_consonants(candidate) == consonants and len(candidate) < len(word_lower) + 3:\n",
    "                return candidate, \"Consonant pattern matching\"\n",
    "    \n",
    "    return word_lower, \"No match found\"\n",
    "\n",
    "# Test the function with the stuttering results\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CORRECTED STUTTERED WORDS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "if len(stuttering_results) > 0:\n",
    "    print(f\"\\n{'Stuttered Word':<20} {'Corrected Word':<20} {'Correction Method':<30} {'Confidence':<12}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for result in stuttering_results:\n",
    "        stuttered = result['word']\n",
    "        corrected, method = get_correct_word(stuttered)\n",
    "        confidence = result['confidence']\n",
    "        \n",
    "        print(f\"{stuttered:<20} {corrected:<20} {method:<30} {confidence:<12}\")\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    # Create corrected transcript\n",
    "    corrected_transcript = []\n",
    "    for word_result in results:\n",
    "        is_valid = is_valid_english_word(word_result['word'])\n",
    "        if not is_valid:\n",
    "            corrected_word, _ = get_correct_word(word_result['word'])\n",
    "            corrected_transcript.append(corrected_word)\n",
    "        else:\n",
    "            corrected_transcript.append(word_result['word'])\n",
    "    \n",
    "    print(f\"\\nOriginal transcript: {' '.join(all_words)}\")\n",
    "    print(f\"Corrected transcript: {' '.join(corrected_transcript)}\")\n",
    "else:\n",
    "    print(\"\\nNo stuttering detected. Transcript is already correct.\")\n",
    "    print(f\"Transcript: {' '.join(all_words)}\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a5ce0",
   "metadata": {},
   "source": [
    "## Generate Audio Pronunciation of Corrected Words\n",
    "\n",
    "Use Google Text-to-Speech (gTTS) to generate proper pronunciation of the corrected stuttering words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db8e8d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "GENERATING PRONUNCIATIONS FOR CORRECTED WORDS\n",
      "====================================================================================================\n",
      "\n",
      "Generating audio pronunciations...\n",
      "\n",
      "✓ Generated pronunciation: really -> ../audio_pronunciations/really_pronunciation.mp3\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Playable Pronunciations:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Stuttered: RREALLY\n",
      "Corrected: really\n",
      "Pronunciation file: ../audio_pronunciations/really_pronunciation.mp3\n",
      "Correction method: Fuzzy matching\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/mpeg;base64,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\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "All pronunciation files saved to: ../audio_pronunciations/\n",
      "Total pronunciations generated: 1\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "from IPython.display import Audio\n",
    "import os\n",
    "\n",
    "def generate_pronunciation(word, output_dir=\"../audio_pronunciations\"):\n",
    "    \"\"\"\n",
    "    Generate audio pronunciation of a word using Google Text-to-Speech\n",
    "    \n",
    "    Args:\n",
    "        word: The word to pronounce\n",
    "        output_dir: Directory to save audio files\n",
    "    \n",
    "    Returns:\n",
    "        path to the audio file\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create filename\n",
    "    filename = f\"{output_dir}/{word.lower()}_pronunciation.mp3\"\n",
    "    \n",
    "    try:\n",
    "        # Generate speech using gTTS\n",
    "        tts = gTTS(text=word, lang='en', slow=False)\n",
    "        tts.save(filename)\n",
    "        print(f\"✓ Generated pronunciation: {word} -> {filename}\")\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error generating pronunciation for '{word}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate pronunciations for all stuttering words\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"GENERATING PRONUNCIATIONS FOR CORRECTED WORDS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "if len(stuttering_results) > 0:\n",
    "    print(f\"\\nGenerating audio pronunciations...\\n\")\n",
    "    \n",
    "    pronunciation_files = {}\n",
    "    \n",
    "    for result in stuttering_results:\n",
    "        stuttered = result['word']\n",
    "        corrected, method = get_correct_word(stuttered)\n",
    "        \n",
    "        # Generate pronunciation for corrected word\n",
    "        audio_file = generate_pronunciation(corrected)\n",
    "        \n",
    "        if audio_file:\n",
    "            pronunciation_files[corrected] = audio_file\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*100)\n",
    "    print(\"\\nPlayable Pronunciations:\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    for result in stuttering_results:\n",
    "        stuttered = result['word']\n",
    "        corrected, method = get_correct_word(stuttered)\n",
    "        \n",
    "        if corrected in pronunciation_files:\n",
    "            print(f\"\\nStuttered: {stuttered}\")\n",
    "            print(f\"Corrected: {corrected}\")\n",
    "            print(f\"Pronunciation file: {pronunciation_files[corrected]}\")\n",
    "            print(f\"Correction method: {method}\")\n",
    "            \n",
    "            # Display audio player\n",
    "            try:\n",
    "                display(Audio(pronunciation_files[corrected]))\n",
    "            except:\n",
    "                print(f\"(Audio file saved but cannot display in this environment)\")\n",
    "    \n",
    "    print(f\"\\n\" + \"-\"*100)\n",
    "    print(f\"\\nAll pronunciation files saved to: ../audio_pronunciations/\")\n",
    "    print(f\"Total pronunciations generated: {len(pronunciation_files)}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nNo stuttering detected. No pronunciations needed.\")\n",
    "\n",
    "print(\"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
